# MCP Chatbot Configuration

# WebSocket Chat Configuration
chat:
  # WebSocket server configuration
  websocket:
    host: "localhost"
    port: 8000
    endpoint: "/ws/chat"

    # CORS settings
    allow_origins: ["*"]
    allow_credentials: true

    # WebSocket settings
    max_message_size: 16777216  # 16MB
    ping_interval: 20
    ping_timeout: 10

  # Chat Storage Configuration
  storage:
    # Repository type: 'auto_persist' (default) or 'memory'
    type: "auto_persist"
    
    # Auto-persistence settings (only used when type is 'autopersist')
    persistence:
      db_path: "chat_history.db"
      retention:
        max_age_hours: 24     # null for indefinite (messages older than X hours)
        max_messages: 1000    # null for unlimited (total messages across all sessions)
        max_sessions: 2      # null for unlimited (number of conversation sessions)
        cleanup_interval_minutes: 2  # How often to run cleanup
      
    # Manual save feature (saves entire sessions)
    saved_sessions:
      enabled: true
      retention_days: null    # null for indefinite
      max_saved: 50          # Maximum number of saved sessions

  # Chat Service Configuration
  service:
    # System prompt configuration
    system_prompt: |
      You are a helpful assistant with a sense of humor.
      You have access to to a list of tools like setting your own configuration. 

    # Streaming configuration
    streaming:
      enabled: true  # Enable streaming by default (required setting)

    # Tool execution configuration
    max_tool_hops: 8  # Maximum number of recursive tool calls to prevent infinite loops

    # Tool execution notifications
    tool_notifications:
      enabled: true
      show_args: true
      icon: "ðŸ”§"
      format: "{icon} Executing tool: {tool_name}"
      # Available placeholders: {icon}, {tool_name}, {tool_args}

    # Logging configuration for chat service
    logging:
      tool_execution: true
      tool_results: true
      result_truncate_length: 200
      system_prompt: true  # Log the generated system prompt during initialization
      llm_replies: true  # Log ALL LLM replies including internal ones not sent to user
      llm_reply_truncate_length: 500  # Truncate length for LLM reply logs

# LLM Configuration - Ready for ANY parameters and reasoning models!
llm:
  active: "openrouter"  # Change this to: openai, groq, openrouter

  # Provider presets - Add ANY parameter and it will be passed through
  providers:
    openai:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"  # or gpt-4, gpt-4-turbo, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      # Any future OpenAI parameter can be added here

    # Example: OpenAI o1 reasoning model configuration
    openai_reasoning:
      base_url: "https://api.openai.com/v1"
      model: "o1-preview"  # Reasoning model
      # o1 models use different parameters
      max_completion_tokens: 8192  # o1 uses this instead of max_tokens
      # temperature is fixed at 1 for o1 models (don't include it)
      
    groq:
      base_url: "https://api.groq.com/openai/v1"
      model: "llama-3.3-70b-versatile"  # or llama-3.1-8b-instant, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      # Groq-specific parameters
      response_format: {"type": "text"}  # Could be "json_object" for JSON responses

    # Example: OpenRouter with various models and parameters
    openrouter:
      base_url: "https://openrouter.ai/api/v1"
      model: "openai/gpt-4o-mini"  # Standard model
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      # OpenRouter-specific parameters
      transforms: ["middle-out"]  # OpenRouter transform options

    # Example: OpenRouter with o3 reasoning model
    openrouter_reasoning:
      base_url: "https://openrouter.ai/api/v1"
      model: "openai/o3-mini"  # Latest reasoning model
      temperature: 0.7
      max_tokens: 8192
      # Reasoning-specific parameters (these are examples)
      reasoning_effort: "high"  # low, medium, high
      include_thinking: true    # Whether to include thinking in response
      
    # Example: Anthropic Claude with thinking mode (if available via OpenRouter)
    anthropic_thinking:
      base_url: "https://openrouter.ai/api/v1"
      model: "anthropic/claude-3-opus"
      temperature: 0.7
      max_tokens: 4096
      # Potential thinking mode parameters (examples)
      show_reasoning: true
      thinking_mode: "step_by_step"
      
    # Example: Custom provider configuration
    custom_provider:
      base_url: "https://api.example.com/v1"
      model: "custom-model-v2"
      temperature: 0.8
      max_tokens: 2048
      # Any custom parameters the provider supports
      custom_param1: "value1"
      custom_param2: 42
      nested_config:
        sub_param: "nested_value"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"

# MCP Server Configuration
mcp:
  config_file: "servers_config.json"
  
  # Connection and retry configuration
  connection:
    # Maximum number of reconnection attempts per server
    max_reconnect_attempts: 5
    
    # Initial delay between reconnection attempts (seconds)
    # Uses exponential backoff up to max_reconnect_delay
    initial_reconnect_delay: 1.0
    
    # Maximum delay between reconnection attempts (seconds)
    max_reconnect_delay: 30.0
    
    # Connection timeout for initial server connection (seconds)
    connection_timeout: 30.0
    
    # Ping timeout for connection health checks (seconds)
    ping_timeout: 10.0
