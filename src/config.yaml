# MCP Chatbot Configuration

# WebSocket Chat Configuration
chat:
  # WebSocket server configuration
  websocket:
    host: "localhost"
    port: 8000
    endpoint: "/ws/chat"

    # CORS settings
    allow_origins: ["*"]
    allow_credentials: true

    # WebSocket settings
    max_message_size: 16777216  # 16MB
    ping_interval: 20
    ping_timeout: 10

  # Chat Storage Configuration
  storage:
    # Auto-persistence (always on)
    persistence:
      db_path: "chat_history.db"
      retention:
        max_age_hours: 24     # null for indefinite (messages older than X hours)
        max_messages: 1000    # null for unlimited (total messages across all sessions)
        max_sessions: 2      # null for unlimited (number of conversation sessions)
        cleanup_interval_minutes: 2  # How often to run cleanup
      
    # Manual save feature (saves entire sessions)
    saved_sessions:
      enabled: true
      retention_days: null    # null for indefinite
      max_saved: 50          # Maximum number of saved sessions

  # Chat Service Configuration
  service:
    # System prompt configuration
    system_prompt: |
      You are a helpful assistant with a sense of humor.
      You have access to to a list of tools, use them when needed and explain why you used them.

    # Streaming configuration
    streaming:
      enabled: true  # Enable streaming by default (required setting)

    # Tool execution configuration
    max_tool_hops: 8  # Maximum number of recursive tool calls to prevent infinite loops

    # Tool execution notifications
    tool_notifications:
      enabled: true
      show_args: true
      icon: "ðŸ”§"
      format: "{icon} Executing tool: {tool_name}"
      # Available placeholders: {icon}, {tool_name}, {tool_args}

    # Logging configuration for chat service
    logging:
      tool_execution: true
      tool_results: true
      result_truncate_length: 200
      system_prompt: true  # Log the generated system prompt during initialization
      llm_replies: true  # Log ALL LLM replies including internal ones not sent to user
      llm_reply_truncate_length: 500  # Truncate length for LLM reply logs

# LLM Configuration - Just change the 'active' provider!
llm:
  active: "openrouter"  # Change this to: openai, groq, openrouter

  # Provider presets
  providers:
    openai:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"  # or gpt-4, gpt-4-turbo, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

    groq:
      base_url: "https://api.groq.com/openai/v1"
      model: "llama-3.3-70b-versatile"  # or llama-3.1-8b-instant, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

    openrouter:
      base_url: "https://openrouter.ai/api/v1"
      model: "openai/gpt-4o-mini"  # or other available models
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"

# MCP Server Configuration
mcp:
  config_file: "servers_config.json"
  
  # Connection and retry configuration
  connection:
    # Maximum number of reconnection attempts per server
    max_reconnect_attempts: 5
    
    # Initial delay between reconnection attempts (seconds)
    # Uses exponential backoff up to max_reconnect_delay
    initial_reconnect_delay: 1.0
    
    # Maximum delay between reconnection attempts (seconds)
    max_reconnect_delay: 30.0
    
    # Connection timeout for initial server connection (seconds)
    connection_timeout: 30.0
    
    # Ping timeout for connection health checks (seconds)
    ping_timeout: 10.0
